{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436f8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890851fd",
   "metadata": {},
   "source": [
    "### Residual_block\n",
    "Stage1 에서는 ResNet50 에서 사용하는 Residual block을 동일하게 사용하였습니다.\n",
    "\n",
    "\n",
    "### Basic Residual_block\n",
    "Stage2 이상부터는 단순히 3 * 3 conv를 2개 이어붙힌 Basic Residual block을 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c38a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_block(nn.Module):\n",
    "  def __init__(self, in_channels=256, bottleneck_channels=64):\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=bottleneck_channels, kernel_size=1, stride=1),\n",
    "        nn.BatchNorm2d(bottleneck_channels)\n",
    "    )\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=bottleneck_channels, out_channels=bottleneck_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(bottleneck_channels)\n",
    "    )\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=bottleneck_channels, out_channels=in_channels, kernel_size=1, stride=1),\n",
    "        nn.BatchNorm2d(in_channels)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    skip_connection = x \n",
    "    x = self.layer1(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "\n",
    "    x = self.layer2(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "\n",
    "    x = self.layer3(x)\n",
    "    x += skip_connection\n",
    "    x = F.relu(x)\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class Basic_Residual_block(nn.Module):\n",
    "  def __init__(self, in_channels=256):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
    "    self.batchNorm1 = nn.BatchNorm2d(in_channels)\n",
    "    self.batchNorm2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    skip = x \n",
    "    x = self.conv1(x)\n",
    "    x = self.batchNorm1(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchNorm2(x)\n",
    "    x += skip\n",
    "    x = F.relu(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50152d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 512, 512])\n",
      "torch.Size([1, 256, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(1, 256, 512, 512)\n",
    "print(data.shape)\n",
    "\n",
    "residual = Residual_block()\n",
    "result = residual(data)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef1311",
   "metadata": {},
   "source": [
    "## 다양한 모듈을 입력이 다른 경우 동적으로 관리하기 위해서 nn.ModuleList()를 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d2db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage1 Out shape : torch.Size([4, 50])\n",
      "stage2 Out shape : torch.Size([4, 200])\n"
     ]
    }
   ],
   "source": [
    "'''Practice\n",
    "nn.ModuleList() 사용방법 연습\n",
    "'''\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, channels_info):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    for in_c, out_c in zip(channels_info[ : -1], channels_info[1 : ]):\n",
    "      self.layers.append(nn.Linear(in_c, out_c))\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers :\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "channels_info = [10, 20, 30, 40, 50]\n",
    "model = Model(channels_info)\n",
    "x = torch.randn(4, channels_info[0])\n",
    "out = model(x)\n",
    "print(f'stage1 Out shape : {out.shape}')\n",
    "\n",
    "\n",
    "\n",
    "channels_info = [30, 50, 80, 100, 200]\n",
    "model = Model(channels_info)\n",
    "x = torch.randn(4, channels_info[0])\n",
    "out = model(x)\n",
    "print(f'stage2 Out shape : {out.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b8721",
   "metadata": {},
   "source": [
    "### 각 Stage별로 Exchange block으로 구성되기에 Exchange block을 선언하여 재사용할 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffea34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exchange_block(nn.Module):\n",
    "  def __init__(self, input_info):\n",
    "    super().__init__()\n",
    "    self.num_branch = len(input_info)\n",
    "    self.branchs = nn.ModuleList()\n",
    "\n",
    "    # stage1 \n",
    "    self.branchs.append(Residual_block(in_channels=input_info[0]))\n",
    "\n",
    "    # stage2 이상 \n",
    "    if self.num_branch > 1 :\n",
    "      for channel in input_info[1 : ] :\n",
    "        self.branchs.append(Basic_Residual_block(in_channels=channel))\n",
    "    \n",
    "    # exchange unit\n",
    "    self.fues_layer = nn.ModuleList()\n",
    "    for j in range(self.num_branch):\n",
    "      fs = nn.ModuleList()\n",
    "      for i in range(self.num_branch):\n",
    "        # i feature map을 j feature map과 동일한 크기로 맞춰주는 과정\n",
    "        if input_info[i] == input_info[j] :\n",
    "          fs.append(nn.Identity())\n",
    "        else :\n",
    "          fs.append(nn.Sequential(\n",
    "              nn.Conv2d(in_channels=input_info[i], out_channels=input_info[j], kernel_size=1),\n",
    "              nn.BatchNorm2d(input_info[j])\n",
    "          ))\n",
    "      self.fues_layer.append(fs)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    features = [branch(x) for branch, x in zip(self.branchs, inputs)] # 각 입력을 각 모듈에 매핑하여, 각각 독립적으로 철힌다.\n",
    "\n",
    "    outputs = []\n",
    "    for j in range(self.num_branch):\n",
    "      fused = 0\n",
    "      for i in range(self.num_branch):\n",
    "        x = features[i] # i번째 스태이지의 특징맵\n",
    "        x = self.fues_layer[j][i](x) # i번쨰 스테이지의 특징맵을 j 번째 특징맵과 동일 차원으로 변경\n",
    "\n",
    "        if i < j : # 저해상도로 바꿔야되는 경우\n",
    "          factor = 2 ** (j - i)\n",
    "          x = F.avg_pool2d(x, kernel_size=factor, stride=factor)\n",
    "        elif i > j : # 고해상도로 바꿔야하는 경우\n",
    "          factor = 2 ** (i - j)\n",
    "          x = F.interpolate(x, scale_factor=factor, mode='nearest')\n",
    "\n",
    "        fused = fused + x\n",
    "      outputs.append(F.relu(fused))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df022f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 0 fused shape: torch.Size([2, 256, 56, 56])\n",
      "Branch 1 fused shape: torch.Size([2, 512, 28, 28])\n",
      "Branch 2 fused shape: torch.Size([2, 1024, 14, 14])\n",
      "Branch 3 fused shape: torch.Size([2, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "input_info = [256, 512, 1024, 2048]\n",
    "block = Exchange_block(input_info)\n",
    "\n",
    "# 이미지의 해상도는 1/2배씩, 차원은 2배씩 증가시킨다.\n",
    "b0 = torch.randn(2, 256, 56, 56)\n",
    "b1 = torch.randn(2, 512, 28, 28)\n",
    "b2 = torch.randn(2, 1024, 14, 14)\n",
    "b3 = torch.randn(2, 2048, 7, 7)\n",
    "\n",
    "out = block([b0, b1, b2, b3])\n",
    "for i, o in enumerate(out):\n",
    "  print(f\"Branch {i} fused shape: {o.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431347f",
   "metadata": {},
   "source": [
    "### 최종적으로 HRNet을구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce6c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRNet(nn.Module):\n",
    "  def __init__(self, in_channels=3, base_channels=64):\n",
    "    super().__init__()\n",
    "\n",
    "    self.stem = nn.Sequential(\n",
    "      nn.Conv2d(in_channels,  base_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(base_channels),\n",
    "      nn.ReLU(inplace=True),\n",
    "\n",
    "      nn.Conv2d(base_channels,    base_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(base_channels),\n",
    "      nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "    c1 = base_channels                \n",
    "    c1_expanded = 4 * c1            \n",
    "\n",
    "    self.stage1 = nn.Sequential(\n",
    "        # 32 → 128 (projection)\n",
    "        nn.Conv2d(c1, c1_expanded, kernel_size=1, bias=False),\n",
    "        nn.BatchNorm2d(c1_expanded),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        # Bottleneck ×4   (128 → 32 → 32 → 128)\n",
    "        *[Residual_block(in_channels=c1_expanded,\n",
    "                         bottleneck_channels=c1)         \n",
    "          for _ in range(4)],\n",
    "        nn.Conv2d(c1_expanded, c1, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(c1),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "    c1 = base_channels   # 예: 32\n",
    "    c2, c3, c4 = 2*c1, 4*c1, 8*c1\n",
    "\n",
    "    self.trans1 = nn.ModuleList([\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c1, c1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(c2, c3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c2, c3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c3, c4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Stage2: 1 block\n",
    "    self.stage2 = nn.ModuleList([ Exchange_block([c1, c2, c3, c4]) ])\n",
    "    # Stage3: 4 blocks\n",
    "    self.stage3 = nn.ModuleList([ Exchange_block([c1, c2, c3, c4]) for _ in range(4) ])\n",
    "    # Stage4: 3 blocks\n",
    "    self.stage4 = nn.ModuleList([ Exchange_block([c1, c2, c3, c4]) for _ in range(3) ])\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.stem(x)            \n",
    "\n",
    "    # Stage1\n",
    "    x = self.stage1(x)           \n",
    "    branches = [trans(x) for trans in self.trans1]\n",
    "\n",
    "    # Stage2\n",
    "    feature_maps = branches\n",
    "    for block in self.stage2:\n",
    "        feature_maps = block(feature_maps)      \n",
    "\n",
    "    # Stage3\n",
    "    for block in self.stage3:\n",
    "        feature_maps = block(feature_maps)\n",
    "\n",
    "    # Stage4\n",
    "    for block in self.stage4:\n",
    "        feature_maps = block(feature_maps)\n",
    "\n",
    "    f0, f1, f2, f3 = feature_maps\n",
    "\n",
    "    return f0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f9a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([1, 32, 56, 56])\n",
      " Branch 0 shape: torch.Size([1, 32, 56, 56])\n",
      " Branch 1 shape: torch.Size([1, 64, 28, 28])\n",
      " Branch 2 shape: torch.Size([1, 128, 14, 14])\n",
      " Branch 3 shape: torch.Size([1, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HRNet(in_channels=3, base_channels=32).to(device)\n",
    "model.eval()\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(\"Final output shape:\", out.shape)\n",
    "with torch.no_grad():\n",
    "    x1 = model.stem(x)\n",
    "    x1 = model.stage1(x1)\n",
    "    branches = [t(x1) for t in model.trans1]\n",
    "for i, b in enumerate(branches):\n",
    "    print(f\" Branch {i} shape:\", b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
