## AAE: [Adversarial Auto Encoder](https://arxiv.org/abs/1511.05644) <br>
간단 정리 : AutoEncoder의 구조와 Advarsarial 학습을 사용하여 두가지의 장점을 모두 살린 생성형 모델입니다.

## DETR: [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872)  <br>
간단 정리 : 객체 탐지의 복잡한 파이프라인을 사용하는 대신 집합 에측이라는 새로운 관점과 트랜스포머 아키텍처를 사용한 간단한 파이프라인을 제안하였습니다. <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-DETR)

## Co-DETR: [https://arxiv.org/abs/2211.12860](https://arxiv.org/abs/2211.12860)  <br>
간단 정리 : DETR에서 양성 쿼리의 수가 적어 효율적인 학습을 할수 없음을 지적하며, 보조 헤드를 통해 인위적으로 양성 쿼리를 만듦으로써 보다 효율적으로 학습하는 방법을 제안하였습니다. <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Co-DETR-DETR-with-Collaborative-Hybrid-Assignments-Training)

## Swin-Transformer: [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030) <br>
간단 정리 : 기존의 ViT에서는 고정된 크기의 이미지, 해상도에 따른 연산량 2차식 증가 문제 제기, 계층적 구조, window와 shifted window 기반의 self-multi-head attention 제안으로 vision task의 백본을 제안  <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows)


## Segment Anything: [https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643) <br>
간단 정리 : segmentation 분야에서 Foundation 모델을 제안하였습니다. Promptable 모델을 활용, 데이터를 얻기 위한 data engine 등을 활용하여 어떠한 Prompt에 대해서도 segment를 진행하는 모델과 데이터를 제안하였습니다.  <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-SA-segment-Anythings)


## End-to-End-Learning-for-self-driving-cars : [https://arxiv.org/abs/1604.07316](https://arxiv.org/abs/1604.07316) <br>
간단 정리 : 2D 이미지 입력과 운전대의 각도 데이터만을 활용하여 CNN 아키텍처를 활용한 End-to-End 자율주행이 가능함을 입증하였습니다. <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-End-to-End-Learning-for-Self-Driving-cars)

# Multi Modal Learning

## Show and Tell: A Neural Image Caption Generator : [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555) <br>
간단 정리 : RNN과 CNN을 결합하여 Image caption task에서 SOTA 모달을 달성하며 서로 다른 모달리티 결합의 가능성을 보여주었습니다. <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Show-and-Tell-A-Neural-Image-Caption-Generator)


## Show, Attend and Tell : Neural Image caption Generation with Visual Attention : [https://arxiv.org/abs/1502.03044](https://arxiv.org/abs/1502.03044) <br>
간단 정리 : 단어 예측시 이미지의 특정 부분에 집중할 수 있도록 attention 매커니즘을 제안하였습니다. 이에 모델이 단어를 예측할때 이미지의 어딜 보고 예측하는지에 대한 인사이트를 제공하였습니다.  <br>
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Show-Attend-and-Tell-Neural-Image-caption-Generation-with-Visual-Attention)


## ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks : [https://arxiv.org/abs/1908.02265](https://arxiv.org/abs/1908.02265) <br>
간단 정리 : 2-stream & co-attention 아키텍처를 활용하여 각 모달리티에 최적화된 특징 추출 후 각 모달리티가 서로 참조하는 구조를 통해서 자연어-이미지의 joint represnetation 사전학습 모델을 제안합니다.
[BLOG](https://velog.io/@seungminchung/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-ViLBERT-Pretraining-Task-Agnostic-Visiolinguistic-Representations)




